{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Mohamed-Amine Baazizi\n",
    "Affiliation: LIP6 - Faculté des Sciences - Sorbonne Université\n",
    "Email: mohamed-amine.baazizi@lip6.fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prise en main de Scala (30 mn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cet exercice illustre les différentes structures de contrôle de Scala présentées en cours. \n",
    "Il permet de comprendre le paradigme fonctionnel : seules les fonctions `map, reduce, flatten, filter, flatMap` sont autorisées.\n",
    "\n",
    "Temps consillé : 30 mn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définir trois fonctions qui prennent en entrée une liste d'entiers `liste` et réalisent les opérations suivantes:\n",
    "* `maxEntiers` retourne le plus grand des entiers de `liste` \n",
    "* `scEntiers` retourne la somme des carrés des entiers de `liste`\n",
    "* `moyEntiers` retourne la moyenne des entiers de `liste`\n",
    "\n",
    "Tester votre réponses en invoquant ces fonctions sur `listeEntiers` définie comme suit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://134.157.24.93:4042\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = local[*], app id = local-1574269481910)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "listeEntiers: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val listeEntiers = List.range(1,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "maxEntiers: (liste: List[Int])Int\r\n",
       "res0: Int = 10\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def maxEntiers(liste:List[Int]) = liste.reduce((a,b)=>if(a>b)a else b)\n",
    "maxEntiers(listeEntiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "meanTupl: (Int, Int) = (55,10)\r\n",
       "res1: Int = 5\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val meanTupl=listeEntiers.map(x=>(x,1)).reduce((a,b)=>(a._1+b._1,a._2+b._2))\n",
    "meanTupl._1/meanTupl._2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scEntiers: (myList: List[Int])Int\r\n",
       "res2: Int = 385\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scEntiers(myList:List[Int])= myList.map(x=>x*x).reduce((a,b)=>a+b)\n",
    "scEntiers(listeEntiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "moyEntiers: (myList: List[Int])Int\r\n",
       "res3: Int = 5\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def moyEntiers(myList:List[Int])={\n",
    "    //myList.reduce((a,b)=>a+b)/myList.length\n",
    "    val meanTupl=myList.map(x=>(x,1)).reduce((a,b)=>(a._1+b._1,a._2+b._2))\n",
    "    meanTupl._1/meanTupl._2\n",
    "    }\n",
    "moyEntiers(listeEntiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soit une liste chaine de caractères construite à l'aide de l'instruction suivante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "listeTemp: List[String] = List(7,2010,04,27,75, 12,2009,01,31,78, 41,2009,03,25,95, 2,2008,04,28,76, 7,2010,02,32,91)\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val listeTemp = List(\"7,2010,04,27,75\", \"12,2009,01,31,78\", \"41,2009,03,25,95\", \"2,2008,04,28,76\", \"7,2010,02,32,91\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaque élément représente un enregistrement fictif de températures avec le format (station, année, mois, température, code_département). On voudrais calculer pour, l'année 2009, le maximum et la moyenne de ses températures.\n",
    "Compléter l'instruction suivante qui permet les transformations et les conversions de type nécessaires pour l'évaluation de ces deux calculs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max des temps 31\n",
      "moy des temps 28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "temp2009: List[Int] = List(31, 25)\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val temp2009 = listeTemp.map(x=>x.split(\",\")).filter(x=>x(1)==\"2009\").map(x=>(x(3).toInt))\n",
    "\n",
    "println(\"max des temps \" + maxEntiers(temp2009))\n",
    "\n",
    "println(\"moy des temps \" + moyEntiers(temp2009))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soit une liste chaine de caractères construite à l'aide de l'instruction suivante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "melange: List[String] = List(1233,100,3,20171010, 1224,22,4,20171009, 100,lala,comedie, 22,loup,documentaire)\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val melange = List(\"1233,100,3,20171010\", \"1224,22,4,20171009\", \"100,lala,comedie\", \"22,loup,documentaire\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deux types d'éléments existent : ceux de la forme (userID, movieID, rating, timestamp) et ceux de la forme (movieID, title, genre). Le domaine des userID est [1000, 2000] et celui des movieID est [0, 100].\n",
    "\n",
    "Il est demandé de construire à partir de `melange` deux listes distinctes :\n",
    "\n",
    "* `notes` contenant les éléments de la forme (userID, movieID, rating, timestamp) et dont le type est (Int, String, Int, Int) \n",
    "* `films` contenant les éléments de la forme (movieID, title, genre) et dont le type est (Int, String, String)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "notes: List[(Int, String, Int, Int)] = List((1233,100,3,20171010), (1224,22,4,20171009))\r\n",
       "films: List[(Int, String, String)] = List((100,lala,comedie), (22,loup,documentaire))\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val notes = melange.map(x=>x.split(\",\")).filter(x=>x(0).toInt>=1000).map(x=>(x(0).toInt,x(1),x(2).toInt,x(3).toInt))\n",
    "val films = melange.map(x=>x.split(\",\")).filter(x=>x(0).toInt<=100).map(x=>(x(0).toInt,x(1),x(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soit une liste personnes contenant des tuples ayant trois attributs décrivant des personnes :\n",
    "\n",
    "* le premier attribut est le nom de la personne;\n",
    "* le second attribut est le type de la personne : etudiant (etu), enseignant (ens) ou inconnu (nan);\n",
    "* la troisième attribut est l'année d'inscription (s'il s'agit d'un étudiant) ou les années d'ancienneté pour les enseignants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "personnes: List[(String, String, Int)] = List((Joe,etu,3), (Lee,etu,4), (Sara,ens,10), (John,ens,5), (Bill,nan,20))\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val personnes = List((\"Joe\", \"etu\", 3), (\"Lee\", \"etu\", 4), (\"Sara\", \"ens\", 10), (\"John\", \"ens\", 5), (\"Bill\", \"nan\",20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définir une classe `Etu(nom:String, annee:Int)` et une classe `Ens(nom:String, annee:Int)`\n",
    "\n",
    "Transformer la liste `personnes` en une liste d'objets de la classe `Etu` ou `Ens` encapsulant les information des tuples en entrée. Par exemple, le tuple `(\"Joe\", \"etu\", 3)` devra être transformé en un objet `Etu(\"Joe\", 3)`.\n",
    "\n",
    "Attention Les personnes de type inconnu ne doivent être dans le résultat!\n",
    "\n",
    "Utiliser le pattern matching pour répondre à cette question.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Etu\r\n",
       "defined class Ens\r\n",
       "res5: List[Product with Serializable] = List(Etu(Joe,3), Etu(Lee,4), Ens(Sara,10), Ens(John,5))\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Etu(nom:String, annee:Int)\n",
    "case class Ens(nom:String, annee:Int)\n",
    "personnes.filter(x=>x._2!=\"nan\").map{\n",
    "    case(nom,\"etu\",annee)=>Etu(nom, annee)\n",
    "    case(nom,\"ens\",annee)=>Ens(nom, annee)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prise en main de Spark RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prendre le temps de lire la documentation Spark sur l'utilisation des RDD\n",
    "https://spark.apache.org/docs/latest/rdd-programming-guide.html\n",
    "\n",
    "et sur l'API RDD\n",
    "https://spark.apache.org/docs/2.1.1/api/scala/index.html#org.apache.spark.rdd.RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créer un répertoire dans votre dossier `tmp` en tapant\n",
    "\n",
    "`mkdir -p /tmp/BDLE/dataset`\n",
    "\n",
    "* Pour le dataset `wordcount` copier en tapant\n",
    "\n",
    " `cp /Infos/bd/spark/bdle/2015/data/wordcount.txt.bz2 /tmp/BDLE/dataset` \n",
    " \n",
    " `cd /tmp/BDLE/dataset`\n",
    " \n",
    " `bunzip2 wordcount.txt.bz2`\n",
    "  \n",
    "  Vérifier que vous avez bien le fichier `wordcount.txt`\n",
    "\n",
    "\n",
    "* Pour le dataset `books` copier en tapant\n",
    "\n",
    "    `cp -r /Infos/bd/spark/dataset/Books /tmp/BDLE/dataset`\n",
    "\n",
    "    `cd /tmp/BDLE/dataset/Books`\n",
    "    \n",
    "     Vérifier que vous avez bien les fichiers \n",
    "     \n",
    "     `books.csv`  `ratings.csv`\tet `users.csv`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1 (30 mn) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cet exercice utilise le dataset `wordcount`\n",
    "\n",
    "Exécuter la commande suivante pour le charger dans la valeur `data` et tester que le chargement marche bien en lisant les 10 premieres lignes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path: String = C:/Users/yousef/Desktop/\r\n",
       "data: org.apache.spark.rdd.RDD[String] = C:/Users/yousef/Desktop/wordcount.txt MapPartitionsRDD[1] at textFile at <console>:26\r\n",
       "res6: Array[String] = Array(de.b %C3%96ffentliches_Recht/_Assex/_Hessen/_Handakte/_Widerspruchsbescheid_1 1 8533, de.b 2:WorldTraveller101 1 5744, de.b A._Einstein,_Ist_die_Tr%C3%A4gheit_eines_K%C3%B6rpers_von_seinem_Energieinhalt_abh%C3%A4ngig%3F_-_Kommentiert_und_erl%C3%A4utert. 1 9290, de.b A_Poem_a_Day/_2._September:_Verg%C3%A4nglichkeit_der_Sch%C3%B6nheit_(Christian_Hoffmann_von_Hoffmannswaldau) 1 23801, de.b Adventskalender_2009 1 13741, de.b Adventskalender_2011 1 13509, de.b Algorithmensammlung:_Graphentheorie:_Dijkstra-Algorithmus 1 8015, de.b Allgemeine_und_Anorganische_Chemie/_Atombau 1 15108, de.b Analysis 1 ..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val path = \"C:/Users/yousef/Desktop/\" \n",
    "val data = sc.textFile(path + \"wordcount.txt\")\n",
    "\n",
    "data.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Remarque* pour partitionner une chaîne de caractères en utilisant le point (.) comme délimiteur à l'aide de la méthode `split()`, il faut protéger le point avec deux backslahs comme suit `split(\"\\\\.\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1. Structurer le contenu de data de sorte à obtenir un tableau de tableaux de chaines de caractères. Ce dernier devra être stocké dans une nouvelle variable nommée list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[2] at map at <console>:26\r\n",
       "res7: Array[Array[String]] = Array(Array(de.b, %C3%96ffentliches_Recht/_Assex/_Hessen/_Handakte/_Widerspruchsbescheid_1, 1, 8533), Array(de.b, 2:WorldTraveller101, 1, 5744), Array(de.b, A._Einstein,_Ist_die_Tr%C3%A4gheit_eines_K%C3%B6rpers_von_seinem_Energieinhalt_abh%C3%A4ngig%3F_-_Kommentiert_und_erl%C3%A4utert., 1, 9290), Array(de.b, A_Poem_a_Day/_2._September:_Verg%C3%A4nglichkeit_der_Sch%C3%B6nheit_(Christian_Hoffmann_von_Hoffmannswaldau), 1, 23801), Array(de.b, Adventskalender_2009, 1, 13741), Array(de.b, Adventskalender_2011, 1, 13509), Array(de.b, Algorithmensammlung:_Graphentheorie:_Dijkstra-Algorithmus, 1, 8015), Array(de.b, Allgemeine_und_Anorganische_Chemie/_Atombau, 1, 15108), Array(..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val list = data.map(x=>x.split(\" \"))\n",
    "\n",
    "list.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2. Afficher les 100 premiers éléments de la 3e colonne de `list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "q2: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[3] at map at <console>:26\r\n",
       "res8: Array[String] = Array(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 5, 1, 3, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val q2 = list.map(x=>x(2))\n",
    "\n",
    "q2.take(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3. Transformer le contenu de `list` en une liste de paires `(mot, nb)` où `mot` correspond à la première colonne de `list` et `nb` sa troisième colonne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "q3: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[4] at map at <console>:26\r\n",
       "res9: Array[(String, Int)] = Array((de.b,1), (de.b,1), (de.b,1), (de.b,1), (de.b,1), (de.b,1), (de.b,1), (de.b,1), (de.b,1), (de.b,1))\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val q3 = list.map(x=>(x(0),x(2).toInt))\n",
    "\n",
    "q3.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4. Grouper les paires par l'attribut `mot` et additionner leur nombres respectifs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "q4: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[5] at reduceByKey at <console>:26\r\n",
       "res10: Array[(String, Int)] = Array((frr,493), (fr.n,959), (en.q,3202), (en.s,84086), (frp.mw,61), (fr.voy,1646), (de.q,721), (frr.mw,130), (frp,538), (fr.v,893))\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val q4 = q3.reduceByKey((a,b)=>a+b)\n",
    "q4.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5. Reprendre les questions Q3 et Q4 en calculant `mot` différemment : \n",
    "\n",
    "désormais, `mot` doit correspondre au préfixe du premier sous-élément de chaque élément de `list`, \n",
    "\n",
    "Exemple\n",
    "* pour `en.d`, mot doit être `en`\n",
    "* pour `fr.d`, mot doit être `fr`, etc. \n",
    "\n",
    "Comparer les résultats avec ceux obtenus précédemment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "q5: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[6] at map at <console>:26\r\n",
       "res11: Array[(String, Int)] = Array((de,1), (de,1), (de,1), (de,1), (de,1), (de,1), (de,1), (de,1), (de,1), (de,1))\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val q5 =list.map(x=>(x(0).split(\"\\\\.\").reduce((a,b)=>a),x(2).toInt))\n",
    "q5.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "q6: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[7] at reduceByKey at <console>:26\r\n",
       "res12: Array[(String, Int)] = Array((frr,623), (frp,599), (fr,1025597), (en,9428786), (de,984606))\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val q6 =q5.reduceByKey((a,b)=>a+b)\n",
    "q6.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2 (60 mn) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour cet exercice, on utilise le jeux de données Books qui renseigne sur des livres (books.csv), des utilsateurs (users.csv) et des notes réalisées par les utilsateurs (ratings.csv).\n",
    "\n",
    "Les schémas des tables sont \n",
    "\n",
    "* `Users (userid: Number, country: Text, age: Number)`\n",
    "* `Books (bookid: Number, titlewords: Number, authorwords: Number, year: Number, publisher: Number)`\n",
    "* `Ratings (userid: Number, bookid: Number, rating: Number)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path: String = C:/Users/yousef/Desktop/Books/\r\n",
       "books_data: org.apache.spark.rdd.RDD[String] = C:/Users/yousef/Desktop/Books/books.csv MapPartitionsRDD[9] at textFile at <console>:29\r\n",
       "users_data: org.apache.spark.rdd.RDD[String] = C:/Users/yousef/Desktop/Books/users.csv MapPartitionsRDD[11] at textFile at <console>:30\r\n",
       "ratings_data: org.apache.spark.rdd.RDD[String] = C:/Users/yousef/Desktop/Books/ratings.csv MapPartitionsRDD[13] at textFile at <console>:31\r\n",
       "res13: Array[String] = Array(userid,bookid,rating, 276747,4780,4, 276747,1837,4, 276747,6277,3, 276762,7819,1, 276762,4885,3, 276772,27222,2, 276772,33829,5, 276772,83629,5, 276786,246867,3)\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val path = \"C:/Users/yousef/Desktop/Books/\" \n",
    "\n",
    "val books_data = sc.textFile(path + \"books.csv\")\n",
    "val users_data = sc.textFile(path + \"users.csv\")\n",
    "val ratings_data = sc.textFile(path + \"ratings.csv\")\n",
    "\n",
    "\n",
    "books_data.take(10)\n",
    "users_data.take(10)\n",
    "ratings_data.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de formater les données, créer une classe pour chaque table puis charger les données de chaque fichier dans une RDD content des objets de la classe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Users\r\n",
       "defined class Books\r\n",
       "defined class Rating\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Users(var userid: Int, var country: String, var age: Int)\n",
    "//a compléter\n",
    "class Books (var bookid: Int, var titlewords: Int, var authorwords: Int, var year: Int, var publisher: Int) \n",
    "class Rating(var userid: Int, var bookid: Int, var rating: Int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Users_header: String = userid,country,age\r\n",
       "Books_header: String = bookid,titlewords,authorwords,year,publisher\r\n",
       "Rating_header: String = userid,bookid,rating\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Users_header=users_data.first()\n",
    "val Books_header=books_data.first()\n",
    "val Rating_header=ratings_data.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque table, créer une RDD contenant des objets de la classe lui correspondant. Compléter les instructions ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "books: List[Books] = List(Books@2789080d, Books@21ca5965, Books@2e4dd114, Books@45e0552f, Books@2a46f4fe, Books@59b481a2, Books@19b12ef1, Books@5ebf49c5, Books@4343a7e8, Books@3e8787b3, Books@2ed1e034, Books@a84654a, Books@54a6ff15, Books@561e502d, Books@77efca9a, Books@5e685f18, Books@3d555d97, Books@196efd28, Books@10dc4f85, Books@c505219, Books@1b7e1fe1, Books@7517261b, Books@7c5ff5b, Books@90bb520, Books@626132f7, Books@2e0ae75e, Books@69dfae3, Books@52ebfda5, Books@39768b85, Books@403fabe2, Books@291c0be4, Books@2417ddb7, Books@29b05013, Books@146d9e7c, Books@5de7f43e, Books@47877050, Books@6c9c807b, Books@147a6dea, Books@76de2d5f, Books@aa7c624, Books@2b01053, Books@456238d2, Books@42d141f4, Books@76f3296c, Books@74439d69, Books@20338f9b, Books@68a5b488, Books@7f248b9, Books@6fb22..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val books = books_data.filter(x=>x!=Books_header).map(x=>x.split(\",\")).collect.toList.map{case Array(b,t,a,y,p)=>new Books(b.toInt,t.toInt,a.toInt,y.toInt,p.toInt)}\n",
    "books.take(10)\n",
    "\n",
    "\n",
    "val users=users_data.filter(x=>x!=Users_header).map(x=>x.split(\",\")).collect.toList.map{case Array(u,c,a)=>new Users(u.toInt,c,a.toInt)}\n",
    "users.take(10)\n",
    "\n",
    "\n",
    "val notes = ratings_data.filter(x=>x!=Rating_header).map(x=>x.split(\",\")).collect.toList.map{case Array(u,b,r)=>new Rating(u.toInt,b.toInt,r.toInt)}\n",
    "notes.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exprimer les requêtes ci-dessous en opérateurs RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requêtes sur une seule table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifiants d'utilisateurs du pays 'france'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s0: List[Users] = List(Users@42b03cb3, Users@201872e7, Users@64d7d9d2, Users@4d749355, Users@1380aa51, Users@4ce3933e, Users@4a921fc9, Users@7b1ddfca, Users@781fcc48, Users@54bc7cd5, Users@558cf30f, Users@66f301df, Users@6088ecc5, Users@57e57ed5, Users@6543fbad, Users@1bc0f323, Users@3c89da5a, Users@406608c8, Users@3a820dfa, Users@26c7b862, Users@546eedea, Users@243e9fe7, Users@ef9bb47, Users@15e60652, Users@3644a8b9, Users@66d5c831, Users@25b5642f, Users@c4225d, Users@13d6539c, Users@5b8a9597, Users@559016d5, Users@74e710f0, Users@558ce743, Users@5aa5e648, Users@fd67577, Users@43fa84ed, Users@717ec52e, Users@49cfaef, Users@385d4ac6, Users@2f176ccc, Users@28c78bb5, Users@4b6f57c0, Users@19362f11, Users@dd351f7, Users@19ddf9db, Users@283d7c77, Users@db90c6, Users@179035c3, Users@617515cb..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val s0 = users.filter(x=>x.country==\"france\")\n",
    "s0.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifiants des livres dont la date est 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s1: List[Int] = List(29, 37, 45, 52, 81, 84, 102, 114, 120, 127, 162, 170, 202, 212, 219, 231, 248, 283, 319, 328, 329, 340, 345, 356, 369, 373, 379, 396, 401, 411, 456, 459, 474, 509, 511, 520, 525, 581, 605, 606, 614, 622, 629, 661, 671, 696, 705, 708, 720, 726, 728, 750, 753, 762, 765, 767, 768, 772, 773, 775, 782, 790, 795, 810, 874, 880, 900, 912, 932, 938, 946, 976, 985, 1065, 1111, 1118, 1157, 1170, 1195, 1203, 1233, 1255, 1283, 1330, 1331, 1352, 1385, 1423, 1470, 1477, 1491, 1506, 1512, 1534, 1555, 1562, 1601, 1629, 1659, 1661, 1680, 1696, 1709, 1718, 1722, 1773, 1779, 1785, 1790, 1803, 1811, 1820, 1850, 1853, 1854, 1855, 1863, 1895, 1912, 1920, 1953, 1999, 2016, 2034, 2042, 2044, 2057, 2072, 2083, 2091, 2180, 2193, 2288, 2289, 2290, 2301, 2314, 2321, 2356, 2357, 2362, 2375, 238..."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var s1 = books.filter(x=>x.year==2000).map(x=>x.bookid)\n",
    "s1.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifiants des livres notés plus que 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s2: List[Int] = List(4780, 1837, 33829, 83629, 19993, 14817, 17085, 34237, 178121, 233882, 1028, 27517, 88357, 15603, 50712, 140338, 82884, 8273, 52802, 5733, 26790, 125302, 67467, 34540, 49233, 34541, 34542, 114615, 51928, 51927, 51925, 15185, 15189, 32211, 10787, 246901, 51928, 51927, 51925, 79832, 225454, 2997, 55603, 5732, 7217, 22200, 3740, 4350, 24410, 66563, 7295, 86299, 5678, 820, 10566, 63410, 103809, 15789, 45316, 38, 82045, 7084, 44442, 15388, 5546, 2252, 16304, 16305, 61557, 61548, 13246, 11672, 19953, 2687, 671, 4727, 9923, 16739, 19224, 106694, 2257, 3267, 37152, 6402, 6348, 1282, 63577, 244056, 2642, 22053, 12367, 5663, 5089, 2231, 148619, 11837, 61749, 7937, 14611, 5650, 10617, 167, 166864, 31644, 13784, 30967, 15699, 486, 44816, 55827, 14360, 3777, 20188, 39769, 138049,..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//notes==ratings\n",
    "var s2 = notes.filter(x=>x.rating>3).map(x=>x.bookid)\n",
    "s2.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requêtes d'agrégation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nombres d'utilisateurs par pays, triés par ordre décroissant de ce nombre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "q1: List[(String, Int)] = List((usa,1), (canada,1), (usa,1), (germany,1), (usa,1), (canada,1), (usa,1), (usa,1), (usa,1), (usa,1), (united kingdom,1), (canada,1), (usa,1), (usa,1), (usa,1), (usa,1), (usa,1), (germany,1), (usa,1), (australia,1), (germany,1), (germany,1), (united kingdom,1), (usa,1), (iran,1), (usa,1), (canada,1), (usa,1), (usa,1), (canada,1), (usa,1), (usa,1), (italy,1), (usa,1), (usa,1), (usa,1), (usa,1), (united kingdom,1), (usa,1), (portugal,1), (canada,1), (usa,1), (usa,1), (usa,1), (usa,1), (usa,1), (spain,1), (germany,1), (usa,1), (usa,1), (usa,1), (usa,1), (usa,1), (usa,1), (usa,1), (singapore,1), (usa,1), (ireland,1), (usa,1), (canada,1), (usa,1), (usa,1), (united kingdom,1), (germany,1), (united kingdom,1), (usa,1), (canada,1), (england,1), (france,1), (united k..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val q1 = users.map(x=>(x.country,1))\n",
    "val rddq1 = sc.parallelize(q1)\n",
    "val qq1=rddq1.reduceByKey(_+_).sortBy(_._2,false)\n",
    "qq1.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res19: Array[(String, Int)] = Array((usa,18935), (canada,2505), (germany,1254), (unknown,1069), (united kingdom,1019), (australia,581), (spain,518), (france,309), (italy,211), (portugal,184))\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qq1.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pays qui a le plus grand nombre d'utilisateurs. Il n y a pas d'ex aequo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "q2: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[27] at map at <console>:28\r\n",
       "res20: Array[String] = Array(usa, canada, germany, unknown, united kingdom, australia, spain, france, italy, portugal)\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val q2 = qq1.map(x=>x._1)\n",
    "q2.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Année avec le plus grand nombre de livres édités. Il n y a pas d'ex aequo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "q3: List[(Int, Int)] = List((2001,1), (1991,1), (1999,1), (1991,1), (1998,1), (1994,1), (1999,1), (1994,1), (1999,1), (2001,1), (1999,1), (2004,1), (1997,1), (2000,1), (1996,1), (2003,1), (1998,1), (2000,1), (1988,1), (2002,1), (1993,1), (1996,1), (1979,1), (1995,1), (2000,1), (1994,1), (2001,1), (2002,1), (2002,1), (1997,1), (2000,1), (1999,1), (1988,1), (1982,1), (1992,1), (1986,1), (1991,1), (1995,1), (1978,1), (1994,1), (1994,1), (1998,1), (1997,1), (2003,1), (1983,1), (2003,1), (1991,1), (1987,1), (1988,1), (1995,1), (2000,1), (1990,1), (2000,1), (2002,1), (1999,1), (1991,1), (2001,1), (2001,1), (2001,1), (1994,1), (1998,1), (2001,1), (2001,1), (2002,1), (2000,1), (1996,1), (1995,1), (1986,1), (1995,1), (2003,1), (1992,1), (1995,1), (1996,1), (1994,1), (2000,1), (2003,1), (2003,1),..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val q3 = books.map(x=>(x.year,1))\n",
    "var drrd = sc.parallelize(q3)\n",
    "var qq3=drrd.reduceByKey(_+_).sortBy(_._2,false)\n",
    "qq3.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res26: Array[(Int, (Int, Int))] = Array((1984,(610,610)), (1940,(3,3)), (2000,(3692,3692)), (1932,(1,1)), (1948,(3,3)), (1972,(88,88)), (1996,(2730,2730)), (1988,(942,942)), (1976,(167,167)), (0,(906,906)))\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qq3.join(qq3).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requêtes avec jointure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Les éditeurs de livres ayant été notés par des utilisateurs habitant en France"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "29: error: value join is not a member of org.apache.spark.rdd.RDD[String]\r",
     "output_type": "error",
     "traceback": [
      "<console>:29: error: value join is not a member of org.apache.spark.rdd.RDD[String]\r",
      "       ratings_data.join(books_data)\r",
      "                    ^",
      ""
     ]
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "var rrdbooks = sc.parallelize(books)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rrdbooks.join(notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Les éditeurs de livres n'ayant pas été notés par des utilisateurs habitant en France"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pour chaque livre, la moyenne d'age des utilisateurs qui l'ont noté"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
