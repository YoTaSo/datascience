{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AMAL_TP6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YoTaSo/datascience/blob/main/deeplearning/lstm_gru_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhvSZ5vakQCW"
      },
      "source": [
        "# Initial setups and imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0lodjp0oqMt"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import gzip\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import optim\n",
        "from google.colab import drive\n",
        "from preprocess import TextDataset\n",
        "import sentencepiece as spm\n",
        "import nltk\n",
        "from nltk import tokenize\n",
        "from collections import namedtuple\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK0q3qG4q7oK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "1871ae08-aca4-404b-b12d-68c7ddf6107a"
      },
      "source": [
        "\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/AMAL_TP6'\n",
        "os.chdir(root_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSR271cOEk3W"
      },
      "source": [
        "with open('trump_full_speech.txt', 'r') as f:\n",
        "  text = f.read()\n",
        "#text = text.split()\n",
        "#!pip install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRuSVlw7y2Oz"
      },
      "source": [
        "!pip install sentencepiece\n",
        "!pip install  git+https://github.com/experimaestro/datamaestro git+https://github.com/experimaestro/datamaestro_image git+https://github.com/experimaestro/datamaestro_ml git+https://github.com/experimaestro/datamaestro_text\n",
        "!pip install -U PyYAML\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ag6BppOd3LUU"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaDon2S2Y-rE"
      },
      "source": [
        "\n",
        "class LSTMspeech(torch.nn.Module):\n",
        "  def __init__(self,latent=70,embdDim=100,dimout=2000):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(dimout,embdDim,padding_idx=2)\n",
        "    self.latent=latent\n",
        "    #self.dim=dim\n",
        "    #if multiDim: self.dim=x.size()[1]\n",
        "    #latent=#arg\n",
        "\n",
        "    self.oubliLinear=nn.Linear(latent+embdDim, latent)\n",
        "    self.oublActF=nn.Sigmoid()\n",
        "\n",
        "    self.entreeLinear=nn.Linear(latent+embdDim, latent)\n",
        "    self.entreeActF=nn.Sigmoid()\n",
        "\n",
        "    self.interneLayer=nn.Linear(latent+embdDim, latent)\n",
        "    #self.interneActF=torch.tanh()\n",
        "\n",
        "    self.sortieLinear=nn.Linear(latent+embdDim, latent)\n",
        "    self.sortieActF=nn.Sigmoid()\n",
        "\n",
        "    self.decoder = nn.Linear(latent, dimout)\n",
        "    self.actF=nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    outsLis=[]\n",
        "    lastH=torch.zeros(x.size()[0], self.latent,dtype=torch.float)\n",
        "    lastC=torch.zeros(x.size()[0], self.latent,dtype=torch.float)\n",
        "    for i in range(0,x.size()[1]):\n",
        "      inp=x[:,i]\n",
        "      inp=self.embedding(inp)# batch_size*embd_dim_out\n",
        "\n",
        "      inp=torch.cat((inp,lastH), dim=1).float()#batch_size*(embd_dim_out+latent)\n",
        "\n",
        "      oubli=self.oubliLinear(inp)#batch_size*latent\n",
        "      oubli=self.oublActF(oubli)\n",
        "\n",
        "      entree=self.entreeLinear(inp)#batch_size*latent\n",
        "      entree=self.entreeActF(entree)\n",
        "\n",
        "      interne=self.interneLayer(inp)#batch_size*latent\n",
        "      lastC=oubli*lastC + entree*torch.tanh(interne)#self.interneActF(interne)\n",
        "\n",
        "      sortie=self.sortieLinear(inp)#batch_size*latent\n",
        "      sortie=self.sortieActF(sortie)\n",
        "      \n",
        "      lastH=sortie*torch.tanh(lastC)\n",
        "\n",
        "      outp=self.decoder(lastH)\n",
        "      outp=self.actF(outp)\n",
        "      outsLis.append(outp)\n",
        "    out=torch.stack(outsLis, dim=2)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDPLyDaA-Ic_"
      },
      "source": [
        "\n",
        "class GRUspeech(torch.nn.Module):\n",
        "  def __init__(self,latent=70,embdDim=100,dimout=2000):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(2000,embdDim,padding_idx=2)\n",
        "    self.latent=latent\n",
        "    #self.dim=dim\n",
        "    #if multiDim: self.dim=x.size()[1]\n",
        "    #latent=#arg\n",
        "\n",
        "    self.zLinear=nn.Linear(latent+embdDim, latent,bias=False)\n",
        "    self.zActF=nn.Sigmoid()\n",
        "\n",
        "    self.rLinear=nn.Linear(latent+embdDim, latent,bias=False)\n",
        "    self.rActF=nn.Sigmoid()\n",
        "\n",
        "    self.hLayer=nn.Linear(latent+embdDim, latent,bias=False)\n",
        "    #self.interneActF=torch.tanh()\n",
        "\n",
        "    self.decoder = nn.Linear(latent, dimout)\n",
        "    self.actF=nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    \n",
        "    lastH=torch.zeros(x.size()[0], self.latent,dtype=torch.float)#=500*30\n",
        "    #lastC=torch.zeros(x.size()[0], self.latent,dtype=torch.float)\n",
        "    outsLis=[]\n",
        "    for i in range(0,x.size()[1]):\n",
        "      inp=x[:,i]\n",
        "      embdinp=self.embedding(inp)# batch_size*embd_dim_out=500*100\n",
        "\n",
        "      inp=torch.cat((embdinp,lastH), dim=1).float()#batch_size*(embd_dim_out+latent)500*130\n",
        "\n",
        "      zz=self.zLinear(inp)#batch_size*latent= 500*30\n",
        "      zz=self.zActF(zz)\n",
        "\n",
        "      rr=self.rLinear(inp)#batch_size*latent=500*30\n",
        "      rr=self.rActF(rr)\n",
        "\n",
        "      inpHh=torch.cat((embdinp,rr*lastH), dim=1).float()#500*\n",
        "      #print(inpHh.size())\n",
        "      hh=self.hLayer(inpHh)#batch_size*latent\n",
        "      lastH=(1-zz)*lastH + zz*torch.tanh(hh)#self.interneActF(interne)\n",
        "\n",
        "      outp=self.decoder(lastH)\n",
        "      outp=self.actF(outp)\n",
        "      outsLis.append(outp)\n",
        "    out=torch.stack(outsLis, dim=2)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3DSkugYdLEl"
      },
      "source": [
        "\n",
        "class RNNspeech(torch.nn.Module):\n",
        "  def __init__(self,latent=15,embdDim=100,dimout=2000):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(dimout, embdDim)\n",
        "    self.latent=latent\n",
        "    self.singleLayer = nn.Linear(latent+embdDim, latent)\n",
        "    self.decoder = nn.Linear(latent, dimout)\n",
        "    self.actF=nn.Sigmoid()#nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    \n",
        "    lastH=torch.zeros(x.size()[0], self.latent,dtype=torch.float)\n",
        "    outsLis=[]\n",
        "    for i in range(0,x.size()[1]):\n",
        "      inp=x[:,i]\n",
        "      inp=self.embedding(inp)\n",
        "      inp=torch.cat((inp,lastH), dim=1).float()\n",
        "      inp=self.singleLayer(inp)\n",
        "      inp = torch.tanh(inp)\n",
        "      lastH=inp.float()\n",
        "      outp=self.decoder(lastH)\n",
        "      outp=self.actF(outp)\n",
        "      outsLis.append(outp)\n",
        "    out=torch.stack(outsLis, dim=2).squeeze()\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdkBByUY3TqK"
      },
      "source": [
        "# preparing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1HhqlO3MgTb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f6f80727-e44a-4dde-f3a0-9c26e07def0d"
      },
      "source": [
        "#!pip install sentencepiece\n",
        "vocab_size=1000\n",
        "\n",
        "spm.SentencePieceTrainer.train('--input=myfile.txt --model_prefix=m --vocab_size=1000 --unk_id=0 --bos_id=-1 --eos_id=1 --pad_id=2')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtD_N3A0MkIp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "497cdfe3-bb16-4b50-e06f-a907b0fe9d9b"
      },
      "source": [
        "\n",
        "# makes segmenter instance and loads the model file (m.model)\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "\n",
        "# encode: text => id\n",
        "print(sp.encode_as_pieces('This is a test'))\n",
        "print(sp.encode_as_ids('This is a test'))\n",
        "\n",
        "# decode: id => text\n",
        "print(sp.decode_pieces(['▁This', '▁is', '▁a', '▁t', 'est']))\n",
        "print(sp.decode_ids([156, 22, 11, 248, 277]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁This', '▁is', '▁a', '▁t', 'est']\n",
            "[215, 34, 13, 208, 238]\n",
            "This is a test\n",
            "butd and were Our\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "homenrLQ1Xbe"
      },
      "source": [
        "encoded_text=sp.encode_as_ids(text)\n",
        "encoded_text_tensor=torch.tensor(encoded_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeORNoZCvb4Q"
      },
      "source": [
        "\n",
        "Batch = namedtuple(\"Batch\", \"text\")\n",
        "class myTextDataset(torch.utils.data.Dataset):\n",
        "    \n",
        "    def __init__(self, text: torch.LongTensor, sizes: torch.LongTensor):\n",
        "        self.text = text\n",
        "        self.sizes = sizes\n",
        "        #self.labels = labels\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.sizes)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        return self.text[self.sizes[index]:self.sizes[index+1]]\n",
        "\n",
        "    @staticmethod\n",
        "    def collate(batch):\n",
        "        data = [item for item in batch]\n",
        "        return Batch(torch.nn.utils.rnn.pad_sequence(data, batch_first=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IH-8pfM3vFph",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "1cec5f29-ba0c-473a-e9ac-372d4a2658ab"
      },
      "source": [
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "SentenceSplt=tokenize.sent_tokenize(text)\n",
        "ssizes=np.array([0]+[len(sp.encode_as_ids(x)) for x in SentenceSplt])\n",
        "ssizes=torch.from_numpy(np.cumsum(ssizes))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HdzOZrutSWf"
      },
      "source": [
        "textDatasetObj=myTextDataset(encoded_text_tensor,ssizes)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qfuMQSUzTdd"
      },
      "source": [
        "train_loader = DataLoader(textDatasetObj, collate_fn=myTextDataset.collate, shuffle=True, batch_size=400)\n",
        "#test_loader  = DataLoader( test_ds, collate_fn=myTextDataset.collate, shuffle=True, batch_size=400)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAxtZ7fojqLl"
      },
      "source": [
        "# fiitng DNNs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xE1sq2oB1VwN"
      },
      "source": [
        "#batch learning\n",
        "\n",
        "#torch.manual_seed(42)\n",
        "\n",
        "writer = SummaryWriter()\n",
        "vocab_size=1000\n",
        "#encodedSpech=string2code(text)\n",
        "# Now we can create a model and send it at once to the device\n",
        "model1= LSTMspeech(15,100,vocab_size)\n",
        "model2=GRUspeech(15,100,vocab_size)\n",
        "model3=RNNspeech(15,100,vocab_size)\n",
        "\n",
        "lr = 0.0005\n",
        "n_epochs = 100\n",
        "loss_fn=torch.nn.CrossEntropyLoss( )\n",
        "optimizer1=torch.optim.Adam(model1.parameters(), lr=lr)\n",
        "optimizer2=torch.optim.Adam(model2.parameters(), lr=lr)\n",
        "optimizer3=torch.optim.Adam(model2.parameters(), lr=lr)\n",
        "for t in range(n_epochs):\n",
        "  for X in train_loader:\n",
        "\n",
        "    #X=dataLoader(400)\n",
        "    n=X.text.size()[1]\n",
        "\n",
        "    output1 = model1(X.text[:,0:n-1])\n",
        "    output2 = model2(X.text[:,0:n-1])\n",
        "    output3 = model3(X.text[:,0:n-1])\n",
        "\n",
        "    loss1 = loss_fn(output1,X.text[:,1:n])\n",
        "    loss2 = loss_fn(output2,X.text[:,1:n])\n",
        "    loss3 = loss_fn(output3,X.text[:,1:n])\n",
        "    #if t%100==0:\n",
        "    print(t,\"lstm: \",loss1.item(),\" GRU: \",loss2.item(),\" Rnn: \",loss3.item())\n",
        "\n",
        "    \n",
        "    loss1.backward()\n",
        "    loss2.backward()\n",
        "    loss3.backward()\n",
        "    #print(t,model.singleLayer.weight.grad)\n",
        "    optimizer1.step()\n",
        "    optimizer2.step()\n",
        "    optimizer3.step()\n",
        "\n",
        "    optimizer1.zero_grad()\n",
        "    optimizer2.zero_grad()\n",
        "    optimizer3.zero_grad()\n",
        "\n",
        "    writer.add_scalars('Run2',{'LSTM':loss1.item(),\n",
        "                                'GRU':loss2.item(),\n",
        "                                'RNN':loss3.item()}, t)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuFXieuEaGc0"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}