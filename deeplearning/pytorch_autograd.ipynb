{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP2_Pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YoTaSo/datascience/blob/main/deeplearning/pytorch_autograd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuPOuboqYWaA"
      },
      "source": [
        "#Graphe de calcul, autograd et modules\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.autograd import Function\n",
        "from torch.autograd import gradcheck\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OP8AbhmMa40L"
      },
      "source": [
        "\n",
        "#I use the boston housing data to build the model on\n",
        "url='https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data'\n",
        "df1 = pd.read_csv(url,delim_whitespace=True,header=None)\n",
        "xMat=df1.iloc[:,0:13]\n",
        "yRes=df1.iloc[:,13]\n",
        "augX=[]\n",
        "for i in range(xMat.shape[0]):\n",
        "  b=np.concatenate([[1],xMat.values[i]])\n",
        "  augX.append(b)\n",
        "augX = np.asarray(augX)\n",
        "x_train_tensor = torch.from_numpy(augX).float().to(device)\n",
        "y_train_tensor = torch.from_numpy(yRes.values).float().to(device)\n",
        "y_train_tensor = y_train_tensor.view(-1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLRDfNvmbgET"
      },
      "source": [
        "\n",
        "class Context:\n",
        "    \"\"\"Very simplified context object\"\"\"\n",
        "    def __init__(self):\n",
        "        self._saved_tensors = ()\n",
        "    def save_for_backward(self, *args):\n",
        "        self._saved_tensors = args\n",
        "    @property\n",
        "    def saved_tensors(self):\n",
        "        return self._saved_tensors\n",
        "        from torch.autograd import Function\n",
        "\n",
        "class Linear(Function):\n",
        "  @staticmethod\n",
        "  def forward(ctx, x,w):\n",
        "    ctx.save_for_backward(x,w)\n",
        "    return x.mm(w.t())\n",
        "    \n",
        "  @staticmethod\n",
        "  def backward(ctx, grad_output):\n",
        "    x,w = ctx.saved_tensors\n",
        "    grad_x = grad_w = None\n",
        "    grad_x = grad_output.mm(w)\n",
        "    grad_w = grad_output.t().mm(x)\n",
        "    #grad_b = grad_output.sum(0).squeeze(0)\n",
        "\n",
        "    return grad_x, grad_w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mnmUDxobwCj"
      },
      "source": [
        "linear = Linear()\n",
        "ctx = Context()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42u-H8yKb1fR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "853d20e5-d165-4376-a60c-6ca60f35e229"
      },
      "source": [
        "\n",
        "linear_check = Linear.apply\n",
        "x = torch.randn(506,14,requires_grad=True,dtype=torch.float64,device=device)\n",
        "w = torch.randn(1,14,requires_grad=True,dtype=torch.float64,device=device)\n",
        "torch.autograd.gradcheck(linear_check,(x,w))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEr0g2c_b6Xj"
      },
      "source": [
        "#regression with gradient descent\n",
        "torch.random.manual_seed(3360)\n",
        "w = torch.randn(1,14,requires_grad=True,dtype=torch.float,device=device)\n",
        "learning_rate = 1e-9\n",
        "for t in range(10000):\n",
        "  y_pred = linear.forward(ctx,x_train_tensor,w)\n",
        "  loss = (y_pred - y_train_tensor).pow(2).mean()\n",
        "  if t%50==0:\n",
        "    print(t, loss.item())\n",
        "  loss.backward()\n",
        "  with torch.no_grad():\n",
        "    w -= learning_rate * w.grad\n",
        "  w.grad.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kc73_lALpBHj"
      },
      "source": [
        "############################second part of the TP\n",
        "\n",
        "class MyNeuralNet(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.hidden = nn.Linear(14, 10)\n",
        "    self.output = nn.Linear(10, 1)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.hidden(x)\n",
        "    x = torch.tanh(x)\n",
        "    x = self.output(x)\n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLz49mpPsoC7"
      },
      "source": [
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Now we can create a model and send it at once to the device\n",
        "model = MyNeuralNet().to(device)\n",
        "# We can also inspect its parameters using its state_dict\n",
        "#print(model.state_dict())\n",
        "\n",
        "lr = 1e-5\n",
        "n_epochs = 5000\n",
        "\n",
        "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "for t in range(n_epochs):\n",
        "    # to initilize model\n",
        "    model.train()\n",
        "\n",
        "    # to get prediction by model\n",
        "    yhat = model(x_train_tensor)\n",
        "    \n",
        "    loss = loss_fn(y_train_tensor, yhat)\n",
        "    if t%100==0:\n",
        "      print(t, loss.item())\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UiziHYK2JXe"
      },
      "source": [
        "#########################################\n",
        "#this block didn't work\n",
        "#mynet = nn.Sequential(\n",
        "#    nn.Linear(14, 10),\n",
        "#    nn.Tanh(),\n",
        "#    nn.Linear(10, 1))\n",
        "#\n",
        "#mynet.to(device)\n",
        "#\n",
        "#lr = 1e-4\n",
        "#n_epochs = 5000\n",
        "#\n",
        "#loss_fn = torch.nn.MSELoss(reduction='mean')\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "#\n",
        "#for t in range(n_epochs):\n",
        "#    # to initilize model\n",
        "#    #mynet.train()\n",
        "#\n",
        "#   # to get prediction by model\n",
        "#    yhat = mynet(x_train_tensor)\n",
        "#    \n",
        "#    loss = loss_fn(yhat,y_train_tensor)\n",
        "#    if t%100==0:\n",
        "#      print(t, loss.item())\n",
        "#    mynet.zero_grad()\n",
        "#    loss.backward()\n",
        "#    optimizer.step()\n",
        "#    optimizer.zero_grad()\n",
        "####################################    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8DUnx-J4tM-"
      },
      "source": [
        "#this one worked\n",
        "#with sequential\n",
        "mynet = nn.Sequential(\n",
        "    nn.Linear(14, 10),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(10, 1))\n",
        "\n",
        "mynet.to(device)\n",
        "\n",
        "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
        "learning_rate = 1e-4\n",
        "for t in range(5000):\n",
        "    y_pred = mynet(x_train_tensor)\n",
        "    loss = loss_fn(y_pred, y_train_tensor)\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "    mynet.zero_grad()\n",
        "    loss.backward()\n",
        "    #I don't know why optimizer.step() didn't work\n",
        "    with torch.no_grad():\n",
        "        for param in mynet.parameters():\n",
        "            param -= learning_rate * param.grad"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}